{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "import nltk\n",
    "import spacy\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "import _pickle as cPickle\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'tagger', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(body_data_path, stance_data_path, training=True):\n",
    "    body_data = pd.read_csv(body_data_path, names=['BodyID', 'Body'])\n",
    "    body_data = list(zip(body_data.BodyID.tolist(), body_data.Body.tolist()))[1:]\n",
    "    if training:\n",
    "        stance_data = pd.read_csv(stance_data_path, names=['Headline', 'BodyID', 'Stance'])\n",
    "        stance_data = list(zip(stance_data.Headline.tolist(), stance_data.BodyID.tolist(), stance_data.Stance.tolist()))[1:]\n",
    "    else:\n",
    "        stance_data = pd.read_csv(stance_data_path, names=['Headline', 'BodyID'])\n",
    "        stance_data = list(zip(stance_data.Headline.tolist(), stance_data.BodyID.tolist()))[1:]\n",
    "    id_text = {}\n",
    "    data = []\n",
    "    for _id, text in body_data:\n",
    "        id_text[_id] = text\n",
    "    for row in tqdm(stance_data):\n",
    "        headline = nlp(row[0])\n",
    "        _id = row[1]\n",
    "        if training:\n",
    "            stance = row[2]\n",
    "            data.append((_id, headline, nlp(id_text[_id]), stance))\n",
    "        else:\n",
    "            data.append((_id, headline, nlp(id_text[_id])))\n",
    "    return data\n",
    "\n",
    "def dev_split(data, split=0.8):\n",
    "    n = len(data)\n",
    "    random.shuffle(data)\n",
    "    train_data = data[:int(split*n)]\n",
    "    dev_data = data[int(split*n):]\n",
    "    assert len(train_data)+len(dev_data)==n\n",
    "    return train_data, dev_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refuting_features(hl, body):\n",
    "    _refuting_words = [\n",
    "        'fake',\n",
    "        'fraud',\n",
    "        'hoax',\n",
    "        'false',\n",
    "        'deny', 'denies',\n",
    "        # 'refute',\n",
    "        'not',\n",
    "        'despite',\n",
    "        'nope',\n",
    "        'doubt', 'doubts',\n",
    "        'bogus',\n",
    "        'debunk',\n",
    "        'pranks',\n",
    "        'retract'\n",
    "    ]\n",
    "    hl_lemmas = get_tokenized_lemmas(hl)\n",
    "    features = [1 if word in hl_lemmas else 0 for word in _refuting_words]\n",
    "    return features\n",
    "\n",
    "def get_tokenized_lemmas(s):\n",
    "    return [t.lemma_ for t in s]\n",
    "\n",
    "def binary_co_occurence(hl, body):\n",
    "    # Count how many times a token in the title\n",
    "    # appears in the body text.\n",
    "    bin_count = 0\n",
    "    bin_count_early = 0\n",
    "    for token in hl:\n",
    "        if token.text in body.text:\n",
    "            bin_count += 1\n",
    "        if token.text in body.text[:255]:\n",
    "            bin_count_early += 1\n",
    "    return [bin_count, bin_count_early]\n",
    "\n",
    "def binary_co_occurence_stops(hl, body):\n",
    "    # Count how many times a token in the title\n",
    "    # appears in the body text. Stopwords in the title\n",
    "    # are ignored.\n",
    "    bin_count = 0\n",
    "    bin_count_early = 0\n",
    "    hl = [i for i in hl if i.is_punct == False]\n",
    "    for token in hl:\n",
    "        if token.text in body.text:\n",
    "            bin_count += 1\n",
    "            bin_count_early += 1\n",
    "    return [bin_count, bin_count_early]\n",
    "\n",
    "def polarity_features(headline, body):\n",
    "    _refuting_words = [\n",
    "        'fake',\n",
    "        'fraud',\n",
    "        'hoax',\n",
    "        'false',\n",
    "        'deny', 'denies',\n",
    "        'not',\n",
    "        'despite',\n",
    "        'nope',\n",
    "        'doubt', 'doubts',\n",
    "        'bogus',\n",
    "        'debunk',\n",
    "        'pranks',\n",
    "        'retract'\n",
    "    ]\n",
    "\n",
    "    def calculate_polarity(text):\n",
    "        tokens = get_tokenized_lemmas(text)\n",
    "        return sum([t in _refuting_words for t in tokens]) % 2\n",
    "    features = []\n",
    "    features.append(calculate_polarity(headline))\n",
    "    features.append(calculate_polarity(body))\n",
    "    return features\n",
    "\n",
    "def chargrams(hl, body, size):\n",
    "    chargram_hits = 0\n",
    "    for i in range(len(hl.text)-size+1):\n",
    "        chgram = body.text[i:i+size]\n",
    "        if chgram in body.text:\n",
    "            #print(chgram)\n",
    "            chargram_hits += 1\n",
    "    return chargram_hits\n",
    "\n",
    "def ngrams(hl, body, size):\n",
    "    ngram_hits = 0\n",
    "    for i in range(len(hl.doc)-size+1):\n",
    "        ngram = hl.doc[i:i+size]\n",
    "        if ngram.text in body.text:\n",
    "            #print(ngram)\n",
    "            ngram_hits += 1\n",
    "    return ngram_hits\n",
    "\n",
    "def clean(text):\n",
    "    return ''.join(x.lower_ for x in text)\n",
    "\n",
    "def jaccard_sim(hline, body):\n",
    "    hset = set(clean(hline))\n",
    "    bset = set(clean(body))\n",
    "    if len(bset) == 0.0: return 0.0\n",
    "    else:\n",
    "        return len(hset.intersection(bset))/len(hset.union(bset))\n",
    "\n",
    "\n",
    "def make_features(X):\n",
    "    fvecs = []\n",
    "    for (i, h, b) in tqdm(X):\n",
    "        fvec = []\n",
    "        fvec.append(jaccard_sim(h, b))\n",
    "        fvec.append(ngrams(h, b, 2))\n",
    "        fvec.append(ngrams(h, b, 3))\n",
    "        fvec.append(ngrams(h, b, 4))\n",
    "        fvec.append(ngrams(h, b, 5))\n",
    "        fvec.append(ngrams(h, b, 6))\n",
    "        fvec.append(chargrams(h, b, 2))\n",
    "        fvec.append(chargrams(h, b, 4))\n",
    "        fvec.append(chargrams(h, b, 8))\n",
    "        fvec.append(chargrams(h, b, 16))\n",
    "        fvec.extend(polarity_features(h, b))\n",
    "        fvec.extend(refuting_features(h, b))\n",
    "        fvec.extend(binary_co_occurence(h, b))\n",
    "        fvec.extend(binary_co_occurence_stops(h, b))\n",
    "        fvecs.append(fvec)\n",
    "    return np.array(fvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 39977\n",
      "Dev data: 9995\n",
      "Test data: 25413\n"
     ]
    }
   ],
   "source": [
    "def save_or_load_file(path, obj=None, load=True):\n",
    "    if load:\n",
    "        with open(path, 'rb') as f:\n",
    "            obj = cPickle.load(f)\n",
    "        return obj\n",
    "    else:\n",
    "        with open(path, 'wb') as f:\n",
    "            assert obj is not None\n",
    "            cPickle.dump(obj, f)\n",
    "        \n",
    "train_path = './data/train/train_data.pkl'\n",
    "dev_path = './data/train/dev_data.pkl'\n",
    "test_path = './data/test/test_data.pkl'\n",
    "\n",
    "if os.path.exists(train_path) and os.path.exists(dev_path):\n",
    "    train_data = save_or_load_file(train_path)\n",
    "    dev_data = save_or_load_file(dev_path)\n",
    "else:\n",
    "    train_data, dev_data = dev_split(make_data('./data/train/train_bodies.csv', './data/train/train_stances.csv'))\n",
    "    save_or_load_file(train_path, train_data, False)\n",
    "    save_or_load_file(dev_path, dev_data, False)\n",
    "\n",
    "if os.path.exists(test_path):\n",
    "    test_data = save_or_load_file(test_path)\n",
    "else:\n",
    "    test_data = make_data('./data/test/test_bodies.csv', './data/test/test_stances_unlabeled.csv', False)\n",
    "    save_or_load_file(test_path, test_data, False)\n",
    "\n",
    "print(\"Training data:\", len(train_data))\n",
    "print(\"Dev data:\", len(dev_data))\n",
    "print(\"Test data:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y = [i[:-1] for i in train_data], [i[-1] for i in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('./data/train/train_features.npz'):\n",
    "    feat_train_X = np.load('./data/train/train_features.npz')['arr_0']\n",
    "else:\n",
    "    feat_train_X = make_features(train_X)\n",
    "    np.savez('./data/train/train_features.npz', feat_train_X)\n",
    "        \n",
    "dev_X, dev_y = [i[:-1] for i in dev_data], [i[-1] for i in dev_data]\n",
    "\n",
    "if os.path.exists('./data/train/dev_features.npz'):\n",
    "    feat_dev_X = np.load('./data/train/dev_features.npz')['arr_0']\n",
    "else:\n",
    "    feat_dev_X = make_features(dev_X)\n",
    "    np.savez('./data/train/dev_features.npz', feat_dev_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FakeNet, self).__init__()\n",
    "        self.input = torch.nn.Linear(31, 24)\n",
    "        self.hidden = torch.nn.Linear(24, 12)\n",
    "        self.another_hidden = torch.nn.Linear(12, 6)\n",
    "        #self.yet_another_hidden = torch.nn.Linear(16, 8)\n",
    "        self.last = torch.nn.Linear(6, 4)\n",
    "\n",
    "    def forward(self, X):\n",
    "        if isinstance(X, np.ndarray) or isinstance(X, list):\n",
    "            X = torch.autograd.Variable(torch.FloatTensor(X))\n",
    "        inp_layer = self.input(X)\n",
    "        hidden1 = torch.nn.functional.relu(self.hidden(inp_layer))\n",
    "        hidden2 = torch.nn.functional.relu(self.another_hidden(hidden1))\n",
    "        #hidden3 = torch.nn.functional.relu(self.yet_another_hidden(hidden2))\n",
    "        output = torch.nn.functional.relu(self.last(hidden2))\n",
    "        return output\n",
    "\n",
    "mapping = {'agree': 0, 'discuss':1, 'disagree':2, 'unrelated':3}\n",
    "revmapping = {v:k for k, v in mapping.items()}\n",
    "\n",
    "def train(m, X, y):\n",
    "    opt = torch.optim.Adam(m.parameters(), lr=1e-3)\n",
    "    wts = torch.FloatTensor([1/138, 1/348, 1/36, 1/1478])\n",
    "    for epoch in range(200):\n",
    "        bs = 100\n",
    "        tloss = 0.0\n",
    "        for i in range(0, len(y)-bs+1, bs):\n",
    "            opt.zero_grad()\n",
    "            pred = m.forward(X[i:i+bs]) #prediction on batch features\n",
    "            yb = y[i:i+bs] # batch target\n",
    "            if isinstance(yb, list):\n",
    "                yb = list(map(lambda x: mapping[x], yb)) # str labels to indices\n",
    "                yb = torch.autograd.Variable(torch.LongTensor(yb))\n",
    "            loss = torch.nn.functional.cross_entropy(pred, yb, weight=wts)\n",
    "            tloss += loss.data[0]\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        print(epoch, tloss)\n",
    "def predict(model, X):\n",
    "    return torch.max(torch.nn.functional.log_softmax(model.forward(X)), 1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf = MultinomialNB()\n",
    "#clf = LogisticRegression()\n",
    "clf = SVC(kernel='rbf')\n",
    "#clf = GradientBoostingClassifier(n_estimators=200, random_state=14128, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf.fit(feat_train_X, train_y)\n",
    "print(feat_train_X.shape)\n",
    "#f = FakeNet()\n",
    "#train(f, feat_train_X, train_y)\n",
    "#dev_pred = list(map(lambda x: revmapping[x], list(predict(f, feat_dev_X).data.numpy())))\n",
    "dev_pred = clf.predict(feat_dev_X)\n",
    "print(len(dev_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['agree', 'discuss', 'disagree', 'unrelated']\n",
    "LABELS_RELATED = ['unrelated','related']\n",
    "RELATED = LABELS[0:3]\n",
    "\n",
    "def score_submission(gold_labels, test_labels):\n",
    "    score = 0.0\n",
    "    cm = [[0, 0, 0, 0],\n",
    "          [0, 0, 0, 0],\n",
    "          [0, 0, 0, 0],\n",
    "          [0, 0, 0, 0]]\n",
    "\n",
    "    for i, (g, t) in enumerate(zip(gold_labels, test_labels)):\n",
    "        g_stance, t_stance = g, t\n",
    "        if g_stance == t_stance:\n",
    "            score += 0.25\n",
    "            if g_stance != 'unrelated':\n",
    "                score += 0.50\n",
    "        if g_stance in RELATED and t_stance in RELATED:\n",
    "            score += 0.25\n",
    "\n",
    "        cm[LABELS.index(g_stance)][LABELS.index(t_stance)] += 1\n",
    "    return score, cm\n",
    "\n",
    "def print_confusion_matrix(cm):\n",
    "    lines = []\n",
    "    header = \"|{:^11}|{:^11}|{:^11}|{:^11}|{:^11}|\".format('', *LABELS)\n",
    "    line_len = len(header)\n",
    "    lines.append(\"-\"*line_len)\n",
    "    lines.append(header)\n",
    "    lines.append(\"-\"*line_len)\n",
    "\n",
    "    hit = 0\n",
    "    total = 0\n",
    "    for i, row in enumerate(cm):\n",
    "        hit += row[i]\n",
    "        total += sum(row)\n",
    "        lines.append(\"|{:^11}|{:^11}|{:^11}|{:^11}|{:^11}|\".format(LABELS[i],\n",
    "                                                                   *row))\n",
    "        lines.append(\"-\"*line_len)\n",
    "    print('\\n'.join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = 0.0\n",
    "total = 0.0\n",
    "n = len(dev_data)\n",
    "#predicted = list(clf.predict(feat_dev_X))\n",
    "actual = dev_y\n",
    "score, cm = score_submission(actual, dev_pred)\n",
    "best_score = score_submission(actual, actual)[0]\n",
    "print_confusion_matrix(cm)\n",
    "print(\"Score: \" +str(score) + \" out of \" + str(best_score) + \"\\t(\"+str(round(score*100/best_score, 4)) + \"%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"True distribution\")\n",
    "best_score, cmtrue = score_submission(actual, actual)\n",
    "print_confusion_matrix(cmtrue)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
