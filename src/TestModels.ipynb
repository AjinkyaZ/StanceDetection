{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "import nltk\n",
    "import spacy\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "import _pickle as cPickle\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from feature_engineering import refuting_features, polarity_features, hand_features, gen_or_load_feats\n",
    "from feature_engineering import word_overlap_features\n",
    "from utils.dataset import DataSet\n",
    "from utils.generate_test_splits import kfold_split, get_stances_for_folds\n",
    "from utils.score import report_score, LABELS, score_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(X, name):\n",
    "    h = [i[1] for i in X]\n",
    "    b = [i[2] for i in X]\n",
    "    X_overlap = gen_or_load_feats(word_overlap_features, h, b, \"features/overlap.\"+name+\".npy\")\n",
    "    X_refuting = gen_or_load_feats(refuting_features, h, b, \"features/refuting.\"+name+\".npy\")\n",
    "    X_polarity = gen_or_load_feats(polarity_features, h, b, \"features/polarity.\"+name+\".npy\")\n",
    "    X_hand = gen_or_load_feats(hand_features, h, b, \"features/hand.\"+name+\".npy\")\n",
    "\n",
    "    X = np.c_[X_hand, X_polarity, X_refuting, X_overlap]\n",
    "    return X\n",
    "\n",
    "def make_data(body_data_path, stance_data_path, training=True):\n",
    "    body_data = pd.read_csv(body_data_path, names=['BodyID', 'Body'])\n",
    "    body_data = list(zip(body_data.BodyID.tolist(), body_data.Body.tolist()))[1:]\n",
    "    if training:\n",
    "        stance_data = pd.read_csv(stance_data_path, names=['Headline', 'BodyID', 'Stance'])\n",
    "        stance_data = list(zip(stance_data.Headline.tolist(), stance_data.BodyID.tolist(), stance_data.Stance.tolist()))[1:]\n",
    "    else:\n",
    "        stance_data = pd.read_csv(stance_data_path, names=['Headline', 'BodyID'])\n",
    "        stance_data = list(zip(stance_data.Headline.tolist(), stance_data.BodyID.tolist()))[1:]\n",
    "    id_text = {}\n",
    "    data = []\n",
    "    for _id, text in body_data:\n",
    "        id_text[_id] = text\n",
    "    for row in tqdm(stance_data):\n",
    "        headline = row[0]\n",
    "        _id = row[1]\n",
    "        if training:\n",
    "            stance = row[2]\n",
    "            data.append((_id, headline, id_text[_id], stance))\n",
    "        else:\n",
    "            data.append((_id, headline, id_text[_id]))\n",
    "    return data\n",
    "\n",
    "def dev_split(data, split=0.8):\n",
    "    n = len(data)\n",
    "    random.shuffle(data)\n",
    "    train_data = data[:int(split*n)]\n",
    "    dev_data = data[int(split*n):]\n",
    "    assert len(train_data)+len(dev_data)==n\n",
    "    return train_data, dev_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 39977\n",
      "Dev data: 9995\n",
      "Test data: 25413\n"
     ]
    }
   ],
   "source": [
    "def save_or_load_file(path, obj=None, load=True):\n",
    "    if load:\n",
    "        with open(path, 'rb') as f:\n",
    "            obj = cPickle.load(f)\n",
    "        return obj\n",
    "    else:\n",
    "        with open(path, 'wb') as f:\n",
    "            assert obj is not None\n",
    "            cPickle.dump(obj, f)\n",
    "        \n",
    "train_path = '../data/train/train_data.pkl'\n",
    "dev_path = '../data/train/dev_data.pkl'\n",
    "test_path = '../data/test/test_data.pkl'\n",
    "\n",
    "if os.path.exists(train_path) and os.path.exists(dev_path):\n",
    "    train_data = save_or_load_file(train_path)\n",
    "    dev_data = save_or_load_file(dev_path)\n",
    "else:\n",
    "    train_data, dev_data = dev_split(make_data('../data/train/train_bodies.csv', '../data/train/train_stances.csv'))\n",
    "    save_or_load_file(train_path, train_data, False)\n",
    "    save_or_load_file(dev_path, dev_data, False)\n",
    "\n",
    "if os.path.exists(test_path):\n",
    "    test_data = save_or_load_file(test_path)\n",
    "else:\n",
    "    test_data = make_data('../data/test/test_bodies.csv', '../data/test/test_stances_unlabeled.csv', False)\n",
    "    save_or_load_file(test_path, test_data, False)\n",
    "\n",
    "print(\"Training data:\", len(train_data))\n",
    "print(\"Dev data:\", len(dev_data))\n",
    "print(\"Test data:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39977it [02:57, 224.68it/s]\n",
      "39977it [00:10, 3893.30it/s]\n",
      "39977it [03:03, 217.50it/s]\n",
      "39977it [04:03, 164.11it/s]\n"
     ]
    }
   ],
   "source": [
    "train_X, train_y = [i[:-1] for i in train_data], [i[-1] for i in train_data]\n",
    "feat_train_X = generate_features(train_X, 'train')\n",
    "dev_X, dev_y = [i[:-1] for i in train_data], [i[-1] for i in train_data]\n",
    "feat_dev_X = generate_features(dev_X, 'dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FakeNet, self).__init__()\n",
    "        self.input = torch.nn.Linear(44, 32)\n",
    "        self.hidden = torch.nn.Linear(32, 16)\n",
    "        self.another_hidden = torch.nn.Linear(16, 8)\n",
    "        #self.yet_another_hidden = torch.nn.Linear(16, 8)\n",
    "        self.last = torch.nn.Linear(8, 4)\n",
    "\n",
    "    def forward(self, X):\n",
    "        if isinstance(X, np.ndarray) or isinstance(X, list):\n",
    "            X = torch.autograd.Variable(torch.FloatTensor(X))\n",
    "        inp_layer = self.input(X)\n",
    "        hidden1 = torch.nn.functional.relu(self.hidden(inp_layer))\n",
    "        hidden2 = torch.nn.functional.relu(self.another_hidden(hidden1))\n",
    "        #hidden3 = torch.nn.functional.relu(self.yet_another_hidden(hidden2))\n",
    "        output = torch.nn.functional.relu(self.last(hidden2))\n",
    "        return output\n",
    "\n",
    "    def fit(self, X, y, wts=None):\n",
    "        mapping = {'agree': 0, 'disagree':1, 'discuss':2, 'unrelated':3}\n",
    "        revmapping = {v:k for k, v in mapping.items()}\n",
    "\n",
    "        opt = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        if wts is not None and isinstance(wts, list):\n",
    "            wts = torch.FloatTensor(wts)\n",
    "        for epoch in range(20):\n",
    "            bs = 100\n",
    "            tloss = 0.0\n",
    "            for i in range(0, len(y)-bs+1, bs):\n",
    "                opt.zero_grad()\n",
    "                pred = self.forward(X[i:i+bs]) #prediction on batch features\n",
    "                yb = y[i:i+bs] # batch target\n",
    "                if isinstance(yb, list):\n",
    "                    yb = list(map(lambda x: mapping[x], yb)) # str labels to indices\n",
    "                    yb = torch.autograd.Variable(torch.LongTensor(yb))\n",
    "                if isinstance(yb, np.ndarray):\n",
    "                    yb = torch.autograd.Variable(torch.LongTensor(yb))\n",
    "                loss = torch.nn.functional.cross_entropy(pred, yb, weight=wts)\n",
    "                tloss += loss.data[0]\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "            print(epoch, \"::\", tloss)\n",
    "\n",
    "    def predict(self, X):\n",
    "        result = torch.max(torch.nn.functional.log_softmax(self.forward(X)), 1)[1]\n",
    "        return list(result.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf = MultinomialNB()\n",
    "#clf = LogisticRegression()\n",
    "clf = SVC(kernel='rbf')\n",
    "#clf = GradientBoostingClassifier(n_estimators=200, random_state=14128, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf.fit(feat_train_X, train_y)\n",
    "print(feat_train_X.shape)\n",
    "#f = FakeNet()\n",
    "#train(f, feat_train_X, train_y)\n",
    "#dev_pred = list(map(lambda x: revmapping[x], list(predict(f, feat_dev_X).data.numpy())))\n",
    "dev_pred = clf.predict(feat_dev_X)\n",
    "print(len(dev_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['agree', 'discuss', 'disagree', 'unrelated']\n",
    "LABELS_RELATED = ['unrelated','related']\n",
    "RELATED = LABELS[0:3]\n",
    "\n",
    "def score_submission(gold_labels, test_labels):\n",
    "    score = 0.0\n",
    "    cm = [[0, 0, 0, 0],\n",
    "          [0, 0, 0, 0],\n",
    "          [0, 0, 0, 0],\n",
    "          [0, 0, 0, 0]]\n",
    "\n",
    "    for i, (g, t) in enumerate(zip(gold_labels, test_labels)):\n",
    "        g_stance, t_stance = g, t\n",
    "        if g_stance == t_stance:\n",
    "            score += 0.25\n",
    "            if g_stance != 'unrelated':\n",
    "                score += 0.50\n",
    "        if g_stance in RELATED and t_stance in RELATED:\n",
    "            score += 0.25\n",
    "\n",
    "        cm[LABELS.index(g_stance)][LABELS.index(t_stance)] += 1\n",
    "    return score, cm\n",
    "\n",
    "def print_confusion_matrix(cm):\n",
    "    lines = []\n",
    "    header = \"|{:^11}|{:^11}|{:^11}|{:^11}|{:^11}|\".format('', *LABELS)\n",
    "    line_len = len(header)\n",
    "    lines.append(\"-\"*line_len)\n",
    "    lines.append(header)\n",
    "    lines.append(\"-\"*line_len)\n",
    "\n",
    "    hit = 0\n",
    "    total = 0\n",
    "    for i, row in enumerate(cm):\n",
    "        hit += row[i]\n",
    "        total += sum(row)\n",
    "        lines.append(\"|{:^11}|{:^11}|{:^11}|{:^11}|{:^11}|\".format(LABELS[i],\n",
    "                                                                   *row))\n",
    "        lines.append(\"-\"*line_len)\n",
    "    print('\\n'.join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = 0.0\n",
    "total = 0.0\n",
    "n = len(dev_data)\n",
    "#predicted = list(clf.predict(feat_dev_X))\n",
    "actual = dev_y\n",
    "score, cm = score_submission(actual, dev_pred)\n",
    "best_score = score_submission(actual, actual)[0]\n",
    "print_confusion_matrix(cm)\n",
    "print(\"Score: \" +str(score) + \" out of \" + str(best_score) + \"\\t(\"+str(round(score*100/best_score, 4)) + \"%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"True distribution\")\n",
    "best_score, cmtrue = score_submission(actual, actual)\n",
    "print_confusion_matrix(cmtrue)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
